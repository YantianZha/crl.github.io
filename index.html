<!doctype html>
<html itemscope itemtype="http://schema.org/Event">
<head>
  <title itemprop="name">Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions</title>

  <meta charset="utf-8">
  <meta name="author" content="Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions" />
  <meta name="description" content="ICML 2023 Workshop">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="generator" content="Hugo 0.55.6" />
  <meta property="og:title" content="Workshop on Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions" />
<meta property="og:description" content="CoRL 2023 Workshop" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://yantianzha.github.io/crl.github.io/" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Workshop on Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions"/>
<meta name="twitter:description" content="NeurIPS 2022 Workshop"/>


  <link rel="shortcut icon" href="https://ml4ad.github.io/img/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" href="https://ml4ad.github.io/img/apple-touch-icon.png">

  <link rel="stylesheet" type="text/css" href="https://yantianzha.github.io/crl.github.io/css/main.css">

</head>
<body>
  <div class="global">

    <nav id="nav">
  <ul class="wrapper">
    
      <li class="nav-item">
        
        <a href="#about" title="About" class="nav-link">
          About
        </a>
      </li>
    
      <li class="nav-item">
        
        <a href="#dates" title="Dates" class="nav-link">
          Dates
        </a>
      </li>
    
      <li class="nav-item">
        
        <a href="#speakers" title="Speakers" class="nav-link">
          Speakers
        </a>
      </li>
    
      <li class="nav-item">
        
        <a href="#submit" title="Submit" class="nav-link">
          Submit
        </a>
      </li>
    
      <li class="nav-item">
        
        <a href="#schedule" title="Schedule" class="nav-link">
          Schedule
        </a>
      </li>
    <!--
      <li class="nav-item">
        
        <a href="#challenge" title="Challenge" class="nav-link">
          Challenge
        </a>
      </li>
    -->
      <li class="nav-item">
        
        <a href="#papers" title="Papers" class="nav-link">
          Papers
        </a>
      </li>
<!--    
      <li class="nav-item">
        
        <a href="#location" title="Location" class="nav-link">
          Location
        </a>
      </li>
-->    
      <li class="nav-item">
        
        <a href="#organizers" title="Organizers" class="nav-link">
          Organizers
        </a>
      </li>
    
      <li class="nav-item">
        
        <a href="#sponsors" title="Sponsors" class="nav-link">
          Sponsors
        </a>
      </li>
    
  </ul>
</nav>

<hr>

    

<header class="header">
  <div class="wrapper">
  <!--
    <h1 class="logo-name">
      <a class="logo-link" href="#" title="Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions" itemprop="name">Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions</a>
    </h1>
    
    <h2 class="tagline">6th Nov 2023, Atlanta, GA</h2>
    

    <div class="call-action-area">
        <a href="#papers" class="call-action-link" title="See Accepted Papers">See Accepted Papers</a>
    </div>
-->
      <h1 class="logo-name">
      <a class="logo-link" href="#" title="                                                                                                                             
      







	      

	      
      
      " itemprop="name"> </a>
    </h1>
    
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>    
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>  
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>    
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>  
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>    
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>
    <h2 class="tagline"> </h2>  
  </div>
</header>

<hr>


    <div class="content" id="content">
      <div class="wrapper">
        
          <section class="about" id="about">
            <h2 class="section-title">About</h2>

<p itemprop="description">Welcome to the <a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a> Workshop on Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions!

<p>Humans possess remarkable cognitive abilities that enable them to solve everyday tasks and adapt to new situations. These cognitive functions operate at different levels, including the subconscious (e.g., perception and action), conscious (e.g., planning and reasoning), and introspective (e.g., self-monitoring and self-explanation). Importantly, these levels of cognition are not separate entities but are intricately intertwined and mutually adaptive. Advanced agents like humans rely on higher-level cognitive functions, such as introspection and self-explanation, to guide and inform their lower-level cognitive processes, such as planning and control.</p>

<p>In contrast, current robots lack the conscious and introspective human-level cognitive intelligence that can effectively guide their subconscious intelligence. This absence limits their ability to efficiently perform complex tasks, adapt to novel scenarios, and seamlessly interact with humans. Granting robots similar cognitive intelligence holds significant benefits, including the potential to enhance their capacity for learning, facilitate their adaptation to unforeseen circumstances, and enable more natural and intuitive interactions with human counterparts. One promising hypothesis to achieve this is to establish strong connections between principles derived from cognitive science literature and robot learning.</p>

<p>This workshop seeks to stimulate productive discussions on the underexplored connections between human cognitive procedures and robot learning techniques. By exploring these intersections, we aim to explore new research topics and define new exciting research agendas in the field of robot learning. The workshop will emphasize the identification of new research directions that draw inspiration from cognitive science. The ultimate goal is to enable robots to learn and operate more effectively in real-world settings, potentially alongside humans.</p>


<p>Topics of Interest:<br>
&nbsp;•&nbsp; <b>Robot learning guided by self-explanation or self-introspection:</b> Investigations into methods and algorithms that enable robots to acquire self-explanatory and introspective cognitive abilities, allowing them to understand and improve their own learning processes<br>
&nbsp;•&nbsp; <b>Leveraging world models for robot learning:</b> Approaches that utilize world models, including simulated environments or predictive models, to facilitate robot learning and enhance their adaptability to new scenarios and tasks<br>
&nbsp;•&nbsp; <b>Robot learning with language feedback and/or visual attention:</b> Exploration of techniques that incorporate language feedback or visual attention mechanisms to improve robot learning and enable more effective interaction with the environment and humans<br>
&nbsp;•&nbsp; <b>Learning through interaction with humans:</b> Studies on the integration of human-robot interaction and collaborative learning paradigms, aiming to enhance the robot's learning capabilities through direct engagement and cooperation with human partners<br>
&nbsp;•&nbsp; <b>Robot learning for long-horizon tasks:</b> Investigations into long-term learning approaches that enable robots to acquire and refine skills over extended periods, enabling them to handle complex, multi-stage tasks and maintain performance over time<br>
&nbsp;•&nbsp; <b>Augmenting robot learning using long-term (e.g., physical) memories:</b> Research focused on leveraging long-term memory mechanisms, such as physical storage or other memory structures, that enable enhanced robot learning. This includes investigating approaches that facilitate efficient retrieval of stored knowledge, as well as the ability to update and integrate new information into the existing memory, enabling robots to improve their performance, adaptability, and decision-making in complex scenarios<br>
&nbsp;•&nbsp; <b>Learning multi-robot collaborations:</b> Advancements in the area of multi-robot systems, including cooperative learning, task allocation, communication, and coordination, to enable robots to collaborate and learn from each other in a team-based setting<br>
&nbsp;•&nbsp; <b>Utilizing large language models for improving robot learning:</b> Exploring the application of large language models, such as GPT, to enhance robot learning by leveraging natural language processing and understanding capabilities<br>
</p>


<!--
<p>
<b>Attending:</b></br>
1. <a href="https://nips.cc/Register/view-registration" target="_blank">Register</a> for NeurIPS</br>
2. Authors bring or <a href="https://neurips.cc/FAQ/PrintPostersOnsite" target="_blank">print a poster onsite</a> (use thin paper, not laminated, and no larger than 24 inches wide x 36 inches high)</br>
3a. Attend in person: Come find us in <b>Theater B</b> located on the 2nd floor of the <a href="https://goo.gl/maps/pw94qiZcnub46nWp9" target="_blank">New Orleans Convention Center</a></br>
3b. Attend virtually: Watch talks live from our <a href="https://neurips.cc/virtual/2022/workshop/49981" target="_blank">NeurIPS Portal</a>, ask questions in the "Chat" window, and meet authors at our <a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">Gather Town</a> poster sessions at 10:15, 12:10, 14:10 <a href="https://www.timeanddate.com/time/zones/cst" target="_blank">CST</a>
</p>

<p>
<b>Travel Awards:</b>
you can apply for financial support to attend our workshop <a href="https://docs.google.com/forms/d/e/1FAIpQLSciDVBxYnrCzyyO4FUWYKshUYWh1acfpzvB6DFzvVsgSTeFRw/viewform" target="_blank">here</a>. 
</p>

<p>
<b>Contact:</b> ytzha@umd.edu
</p>


<p>
<b>Post Event:</b> all videos now avilable on <a href="https://slideslive.com/neurips-2022/workshop-machine-learning-for-autonomous-driving" target="_blank">SlidesLive</a> and <a href="https://www.youtube.com/@ml4ad2022/videos" target="_blank">YouTube</a></br>
</p>
-->


          </section>
        
          <section class="dates" id="dates">
            <h2 class="section-title">Dates</h2>

<style> 
hide {
  opacity: 0.0;
}
</style>

<style> 
transparent {
  opacity: 0.7;
}
</style>


<h4>Papers</h4>
<p itemprop="dates-description">


<hide>&rarr;</hide><b>Submission due</b>: 1st October 2023<br> 
<hide>&rarr;</hide><b>Reviewing starts</b>: 2nd October 2023<br>
<hide>&rarr;</hide><b>Reviewing ends</b>: 17th October 2023<br>
<hide>&rarr;</hide><b>Notification</b>: 20th October 2023<br>
<hide>&rarr;</hide><b>Camera Ready</b>: 1st November 2023<br>
<hide>&rarr;</hide><b>Video upload</b>: 1st November 2023<br>
<hide>&rarr;</hide><b>Poster upload</b>: 1st November 2023<br>
</p>

<!--
<h4>Challenge</h4>
<p itemprop="dates-description">
<hide>&rarr;</hide><b>Training materials</b>: 5th September 2022<br>
<hide>&rarr;</hide><b>Open</b>: 26th September 2022<br>
<hide>&rarr;</hide><b>Close</b>: 7th November 2022<br>  
<hide>&rarr;</hide><b>Notification</b>: 10th November 2022<br>
<hide>&rarr;</hide><b>Video submission</b>: 24th November 2022<br>
</p>
-->

<h4>Workshop</h4>
<p itemprop="dates-description">
<hide>&rarr;</hide><b>Workshop event</b>: 6th November 2023<br>
</p>

<script>
  
  var countDownDate = new Date("Oct 01, 2022 23:59:59 UTC").getTime();  
  countDownDate = countDownDate + 1000 * 3600 * 12 

  
  var x = setInterval(function() {

    
    var now = new Date().getTime();

    
    var distance = countDownDate - now;

    
    var days = Math.floor(distance / (1000 * 60 * 60 * 24));
    var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
    var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
    var seconds = Math.floor((distance % (1000 * 60)) / 1000);

    
    var countdown = days + "d " + hours + "h " + minutes + "m " + seconds + "s ";

    
    if (distance < 0) {
      clearInterval(x);
      countdown = "(expired)";  
    }

    document.getElementById("countdown").innerHTML = countdown

  }, 1000);
</script>


          </section>
        
          <section class="speakers" id="speakers">
            <h2 class="section-title">Speakers</h2>

<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {
  box-sizing: border-box;
}

 
.column {
  float: left;
  width: 50%;
  padding: 10px;
}

</style>

<div class="row">
  <div class="column">
    <p>Morning speakers</p><br>
    <ul class="speakers-list">
    
      <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
        <span class="speaker-photo">
          <a href="https://yezhouyang.engineering.asu.edu/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/YezhouYang.jpeg" alt="Yezhou Yang" itemprop="image"></a>
        </span>

        <h3 class="speech-title">      
                  
        </h3>

        <h3 class="speakers-name"><a href="https://yezhouyang.engineering.asu.edu/" target="_blank" title="Yezhou Yang"> Yezhou Yang </a>
        </h3>
        <p class="speakers-bio">Associate Professor<br>Arizona State University</p>
      </li>
    
      <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
        <span class="speaker-photo">
          <a href="https://web.eecs.umich.edu/~jjcorso/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/pic-jcorso.jpg" alt="Jason J. Corso" itemprop="image"></a>
        </span>

        <h3 class="speech-title">
          
           
          
            
          
        </h3>

        <h3 class="speakers-name"><a href="https://web.eecs.umich.edu/~jjcorso/" target="_blank" title="Jason J. Corso"> Jason J. Corso </a>
        </h3>
        <p class="speakers-bio">Professor<br>University of Michigan, Ann Arbor</p>
      </li>
    

    
    
    </ul>
  </div>
  <div class="column">
    <p>Afternoon speakers</p><br>
    <ul class="speakers-list">
    
      <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
        <span class="speaker-photo">
          <a href="https://www.kth.se/profile/dani" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/DanicaKragic.jpg" alt="Danica Kragic" itemprop="image"></a>
        </span>

        <h3 class="speech-title">
          
          
          
          
          
        </h3>

        <h3 class="speakers-name"><a href="https://www.kth.se/profile/dani" target="_blank" title="Danica Kragic"> Danica Kragic </a>
        </h3>
        <p class="speakers-bio">Professor<br>Royal Institute of Technology</p>
      </li>
    
      <li class="speakers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
        <span class="speaker-photo">
          <a href="https://developmental-robotics.jp/en/members/yukie_nagai/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/member_Yukie-Nagai.jpg" alt="Yukie Nagai" itemprop="image"></a>
        </span>

        <h3 class="speech-title">
          
          
          
          
          
        </h3>

        <h3 class="speakers-name"><a href="https://developmental-robotics.jp/en/members/yukie_nagai/" target="_blank" title="Yukie Nagai"> Yukie Nagai </a>
        </h3>
        <p class="speakers-bio">Professor<br>The University of Tokyo</p>
      </li>
    
         
    </ul>
  </div>
</div>

          </section>
        
          <section class="submit" id="submit">
            <h2 class="section-title">Submissions</h2>

<p itemprop="call-details">
<b>Submission deadline: 1st October 2023</b> at 23:59 Anywhere on Earth<br>
Submission website: <a href="https://" target="_blank">TBD</a><br>
Submission format: either extended abstracts (4 pages) or full papers (8 or 9 pages) anonymously using: <br>
&nbsp;•&nbsp; <a href="https://drive.google.com/file/d/1Ksqw_9OMiCKEiGmoaqn3QCuqcRpgtZ5a/view" target="_blank">CoRL2023_template</a> -- LaTeX template<br>
<!--
&nbsp;•&nbsp; <a href="https://ml4ad.github.io/files/neurips_2022_ml4ad.sty" target="_blank">neurips_2022_ml4ad.sty</a> -- style file for LaTeX 2e<br>
&nbsp;•&nbsp; <a href="https://ml4ad.github.io/files/neurips_2022_ml4ad.pdf" target="_blank">neurips_2022_ml4ad.pdf</a> -- example PDF output<br>
-->
References and appendix should be appended into the same (single) PDF document, and do not count towards the page count.<br>
</p>

<!--
<p>We invite submissions on machine learning applied to autonomous driving, including (but not limited to):<br>
&nbsp;•&nbsp; Robot learning guided by self-explanation or self-introspection<br>
&nbsp;•&nbsp; Leveraging world models for robot learning<br>
&nbsp;•&nbsp; Robot learning with language feedback and/or visual attention<br>
&nbsp;•&nbsp; Learning through interaction with humans<br>
&nbsp;•&nbsp; Robot learning for long-horizon tasks<br>
&nbsp;•&nbsp; Robot learning augmented by using long-term (e.g. physical) memories<br>
&nbsp;•&nbsp; Learning multi-robot collaborations<br>
</p>
-->

<p>
<b>FAQ</b><br>
<br>
<b>Q:</b> Will there be archival proceedings?<br>
<b>A:</b> No. Neither 4 or 8 or 9 page submissions will be indexed nor have archival proceedings.<br>
<br>
<b>Q:</b> Should submitted papers be anonymized?<br>
<b>A:</b> Yes. If accepted, we will ask for a de-anonymized version to link on the website.<br>
<br>
<b>Q:</b> My papers contains ABC, but not XYZ, is this good enough for a submission?<br>
<b>A:</b> Submissions will be evaluated based on <a href="https://ml4ad.github.io/img/review-questions.png" target="_blank">these</a> reviewer questions.<br>
<br>

</p>





          </section>
        
          <section class="schedule" id="schedule">
            <h2 class="section-title">Schedule (Tentative)</h2>

<p>Monday, November 6th, 2023. All times are in Central Standard Time (UTC-6). Current time is <span id="centraltime"></span>

</p>

<div class="schedule-tbl">

  

  <table>
    <thead>
      <tr>
        <th class="schedule-time">Time</th>
        <th class="schedule-slot">Event</th>
        <th class="schedule-description">Title</th>
      </tr>
    </thead>
    <tbody>
      
        
          <tr>
            <td class="schedule-time">09:00</td>
            <td class="schedule-slot">
            

              
            
              <a href=https:// target="_blank">Welcome</a>
            
              <span class="speakers-company"></span>
            </td>
            
              <td class="schedule-description"></td>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">9:10</td>
            <td class="schedule-slot">
            
              <span class="speaker-photo">
                <img class="photo" src="https://yantianzha.github.io/crl.github.io/img/pic-jcorso.jpg" alt="Jason J. Corso">
              </span>
            

              
            
              Jason J. Corso
            
              <span class="speakers-company">University of Michigan, Ann Arbor</span>
            </td>
            
              
              <td class="schedule-description"><a href=https:// target="_blank">Learning Representations from Text and Visual Data for Real-Time Perception in Robotics and Augmented Reality</a>
            
          </tr>
          
                  
          <tr>
            <td class="schedule-time">09:50</td>
            <td class="schedule-slot">
            
              <span class="speaker-photo">
                <img class="photo" src="https://yantianzha.github.io/crl.github.io/img/YezhouYang.jpeg" alt="Yezhou Yang ">
              </span>
            

              
            
              Yezhou Yang 
            
              <span class="speakers-company">Arizona State University</span>
            </td>
            
              
              <td class="schedule-description"><a href=https:// target="_blank">Robust and Compositional Concept Grounding for Lifelong Robot Learning</a>
            
          </tr>
        
      

        

          <tr>
            <td class="schedule-time">10:30</td>
            <td class="schedule-slot">            
              Break            
              <span class="speakers-company"></span>
            </td>
            <td class="schedule-description"><a href=https://app.gather.town/app/ target="_blank">Gather Town</a>
          </tr>
        
          <tr>
            <td class="schedule-time">10:40</td>
            <td class="schedule-slot">
            
              Spotlight Talks
            
              <span class="speakers-company"></span>
            </td>
            
              
              <td class="schedule-description"><a href=https://youtu.be target="_blank">Compilation of 10 minutes spotlight talks from all authors</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">12:00</td>
            <td class="schedule-slot">
            

              
            
              Posters and Lunch
            
              <span class="speakers-company"></span>
            </td>
            
              
              <td class="schedule-description"><a href=https://app.gather.town/app/ target="_blank">Gather Town</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">13:30</td>
            <td class="schedule-slot">
            
              <span class="speaker-photo">
                <img class="photo" src="https://yantianzha.github.io/crl.github.io/img/DanicaKragic.jpg" alt="Danica Kragic">
              </span>
            

              
            
              Danica Kragic
            
              <span class="speakers-company">Royal Institute of Technology</span>
            </td>
            
              
              <td class="schedule-description"><a href=https://slideslive.com/ target="_blank">Representation Learning and Representation Evaluation in Robotics</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">14:10</td>
            <td class="schedule-slot">
            
              <span class="speaker-photo">
                <img class="photo" src="https://yantianzha.github.io/crl.github.io/img/member_Yukie-Nagai.jpg" alt="Yukie Nagai">
              </span>
            

              
            
              Yukie Nagai
            
              <span class="speakers-company">The University of Tokyo</span>
            </td>
            
              
              <td class="schedule-description"><a href=https://slideslive.com target="_blank">Predictive Processing for Robot Learning: What we can learn from cognitive science?</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">14:50</td>
            <td class="schedule-slot">                
            
              Break
            
              <span class="speakers-company"></span>
            </td>
            
              
              <td class="schedule-description"><a href=https://app.gather.town/app/ target="_blank">Gather Town</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">15:00</td>
            <td class="schedule-slot">
	       Spotlight Talks
            
              <span class="speakers-company"></span>
            </td>
            
              
              <td class="schedule-description"><a href=https://youtu.be target="_blank">Compilation of 10 minutes spotlight talks from all authors</a>
            
          </tr>
        
      
        
          <tr>
            <td class="schedule-time">16:00</td>
            <td class="schedule-slot">
            Panel Discussion
           
              <span class="speakers-company"></span>
            </td>
            
              
              <td class="schedule-description"><a href=https://youtu.be target="_blank">Streaming</a>
            
          </tr>
        

          <tr>
            <td class="schedule-time">16:55</td>
            <td class="schedule-slot">
            

              
            
              <a href=https://slideslive.com target="_blank">Closing Remarks</a>
            
              <span class="speakers-company"></span>
            </td>
            
              <td class="schedule-description"></td>
            
          </tr>
        
      
    </tbody>
  </table>
</div>



<script>
  
  var x = setInterval(function() {
    var d = new Date();
    var n = d.toLocaleTimeString("en-US", {timeZone: "America/Chicago", hour: '2-digit', minute:'2-digit', hour12: false})
    document.getElementById("centraltime").innerHTML = n
  }, 1000);
</script>


          </section>

<!--        
          <section class="challenge" id="challenge">
            <h2 class="section-title">Challenge</h2>



<br>
<center><p><img src="https://ml4ad.github.io/img/challenge.jpg"><br><br>The CARLA Autonomous Driving Challenge 2022 winners will present their solutions as part of the workshop. Details <a href="https://leaderboard.carla.org/challenge/" target="_blank">here</a>.</p></center>







          </section>
-->
          <section class="papers" id="papers">
            
<h2 class="section-title">Papers</h2>

<!--
<br><br>
<h4 class="section-title">Extended Abstracts</h4>
<br>

<p>

<b>Risk Perception in Driving Scenes</b><br>
Nakul Agarwal, Yi-Ting Chen<br>
<a href="https://ml4ad.github.io/files/papers2022/Risk Perception in Driving Scenes.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B2</a> | 
<a href="https://youtu.be/7luDUtEwNN0" target="_blank">youtube</a>
<br><br> 

<b>Multi-Modal 3D GAN for Urban Scenes</b><br>
Loïck Chambon, Mickael Chen, Tuan-Hung Vu, Alexandre Boulch, Andrei Bursuc, Matthieu Cord, Patrick Pérez<br>
<a href="https://ml4ad.github.io/files/papers2022/Multi-Modal 3D GAN for Urban Scenes.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B9</a> | 
<a href="https://slideslive.com/38993736/multimodal-gan-for-3d-urban-scenes" target="_blank">slideslive</a>
<br><br> 

<b>DriveCLIP: Zero-Shot Transfer for Distracted Driving Activity Understanding using CLIP</b><br>
Md Zahid Hasan, Ameya Joshi, Mohammed Rahman, Archana Venkatachalapathy, Anuj Sharma, Chinmay Hegde, Soumik Sarkar<br>
<a href="https://ml4ad.github.io/files/papers2022/DriveCLIP: Zero-Shot Transfer for Distracted Driving Activity Understanding using CLIP.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C2</a> | 
<a href="https://slideslive.com/38993725/driveclip-zeroshot-transfer-for-distracted-driving-activity-understanding-using-clip" target="_blank">slideslive</a>
<br><br> 

<b>Are All Vision Models Created Equal? A Study of the Open-Loop to Closed-Loop Causality Gap</b><br>
Mathias Lechner, Ramin Hasani, Alexander Amini, Tsun-Hsuan Wang, Thomas Henzinger, Daniela Rus<br>
<a href="https://ml4ad.github.io/files/papers2022/Are All Vision Models Created Equal. A Study of the Open-Loop to Closed-Loop Causality Gap.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C9</a> | 
<a href="https://slideslive.com/38993710/are-all-vision-models-created-equal-a-study-of-the-openloop-to-closedloop-causality-gap" target="_blank">slideslive</a>
<br><br> 

<b>Rationale-aware Autonomous Driving Policy utilizing Safety Force Field implemented on CARLA Simulator</b><br>
Ho Suk*, Taewoo Kim*, Hyungbin Park, Pamul Yadav, Junyong Lee, Shiho Kim<br>
<a href="https://ml4ad.github.io/files/papers2022/Rationale-aware Autonomous Driving Policy utilizing Safety Force Field implemented on CARLA Simulator.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B1</a> | 
<a href="https://slideslive.com/38993708/rationaleaware-autonomous-driving-policy-utilizing-safety-force-field-implemented-on-carla-simulator" target="_blank">slideslive</a> | 
<a href="https://youtu.be/_OL0SUx2JeI" target="_blank">youtube</a>
<br><br> 

</p>


<h4 class="section-title">Full Papers</h4>
<br>

<p>

<b>Monitoring of Perception Systems: Deterministic, Probabilistic, and Learning-based Fault Detection and Identification</b><br>
Pasquale Antonante, Heath Nilsen, Luca Carlone<br>
<a href="https://ml4ad.github.io/files/papers2022/Monitoring of Perception Systems: Deterministic, Probabilistic, and Learning-based Fault Detection and Identification.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B7</a> | 
<a href="https://slideslive.com/38993718/monitoring-of-perception-systems-deterministic-probabilistic-and-learningbased-fault-detection-and-identification" target="_blank">slideslive</a> | 
<a href="https://youtu.be/NHPk6Y6kuAE" target="_blank">youtube</a>
<br><br> 

<b>VN-Transformer: Rotation-Equivariant Attention for Vector Neurons</b><br>
Serge Assaad, Carlton Downey, Rami Al-Rfou, Nigamaa Nayakanti, Benjamin Sapp<br>
<a href="https://ml4ad.github.io/files/papers2022/VN-Transformer: Rotation-Equivariant Attention for Vector Neurons.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A2</a> | 
<a href="https://slideslive.com/38993728/vntransformer-rotationequivariant-attention-for-vector-neurons" target="_blank">slideslive</a> | 
<a href="https://youtu.be/1KrPzUKwSL8" target="_blank">youtube</a>
<br><br> 

<b>Controlling Steering with Energy-Based Models</b><br>
Mykyta Baliesnyi, Ardi Tampuu, Tambet Matiisen<br>
<a href="https://ml4ad.github.io/files/papers2022/Controlling Steering with Energy-Based Models.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A1</a> | 
<a href="https://youtu.be/QfN11Dp7lnw" target="_blank">youtube</a>
<br><br> 

<b>Verifiable Goal Recognition for Autonomous Driving with Occlusions</b><br>
Cillian Brewitt, Massimiliano Tamborski, Stefano Albrecht<br>
<a href="https://ml4ad.github.io/files/papers2022/Verifiable Goal Recognition for Autonomous Driving with Occlusions.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D6</a> | 
<a href="https://slideslive.com/38993723/verifiable-goal-recognition-for-autonomous-driving-with-occlusions" target="_blank">slideslive</a>
<br><br> 

<b>Robust Trajectory Prediction against Adversarial Attacks</b><br>
Yulong Cao, Danfei Xu, Xinshuo Weng, Zhuoqing Mao, Anima Anandkumar, Chaowei Xiao, Marco Pavone<br>
<a href="https://ml4ad.github.io/files/papers2022/Robust Trajectory Prediction against Adversarial Attacks.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A7</a> | 
<a href="https://slideslive.com/38993740/robust-trajectory-prediction-against-adversarial-attacks" target="_blank">slideslive</a> | 
<a href="https://youtu.be/LIn0eAlhcMg" target="_blank">youtube</a>
<br><br> 

<b>AdvDO: Realistic Adversarial Attacks for Trajectory Prediction</b><br>
Yulong Cao, Chaowei Xiao, Anima Anandkumar, Danfei Xu, Marco Pavone<br>
<a href="https://ml4ad.github.io/files/papers2022/AdvDO: Realistic Adversarial Attacks for Trajectory Prediction.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A6</a> | 
<a href="https://slideslive.com/38993737/advdo-realistic-adversarial-attacks-for-trajectory-prediction" target="_blank">slideslive</a> | 
<a href="https://youtu.be/U9n0ymQ1G3U" target="_blank">youtube</a>
<br><br> 

<b>Improving Motion Forecasting for Autonomous Driving with the Cycle Consistency Loss</b><br>
Titas Chakraborty, Akshay Bhagat, Henggang Cui<br>
<a href="https://ml4ad.github.io/files/papers2022/Improving Motion Forecasting for Autonomous Driving with the Cycle Consistency Loss.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B5</a> | 
<a href="https://slideslive.com/38993738/improving-motion-forecasting-for-autonomous-driving-with-the-cycle-consistency-loss" target="_blank">slideslive</a> | 
<a href="https://youtu.be/NZboJ4IulCs" target="_blank">youtube</a>
<br><br> 

<b>VISTA: VIrtual STereo based Augmentation for Depth Estimation in Automated Driving</b><br>
Bin Cheng, Kshitiz Bansal, Mehul Agarwal, Gaurav Bansal, Dinesh Bharadia<br>
<a href="https://ml4ad.github.io/files/papers2022/VISTA: VIrtual STereo based Augmentation for Depth Estimation in Automated Driving.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B3</a> | 
<a href="https://slideslive.com/38993719/vista-virtual-stereo-based-augmentation-for-depth-estimation-in-automated-driving" target="_blank">slideslive</a> | 
<a href="https://youtu.be/90_i2HWyfuo" target="_blank">youtube</a>
<br><br> 

<b>Finding Safe Zones of Markov Decision Processes Policies</b><br>
Lee Cohen, Yishay Mansour, Michal Moshkovitz<br>
<a href="https://ml4ad.github.io/files/papers2022/Finding Safe Zones of Markov Decision Processes Policies.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C8</a> | 
<a href="https://slideslive.com/38993711/finding-safe-zones-of-markov-decision-processes-policies" target="_blank">slideslive</a> | 
<a href="https://youtu.be/D98VoZ6Rav0" target="_blank">youtube</a>
<br><br> 

<b>One-Shot Learning of Visual Path Navigation for Autonomous Vehicles</b><br>
Zhongying CuiZhu*, Francois Charette*, Amin Ghafourian*, Debo Shi, Matthew Cui, Anjali Krishnamachar, Iman Bozchalooi<br>
<a href="https://ml4ad.github.io/files/papers2022/One-Shot Learning of Visual Path Navigation for Autonomous Vehicles.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A5</a> | 
<a href="https://slideslive.com/38993727/oneshot-learning-of-visual-path-navigation-for-autonomous-vehicles" target="_blank">slideslive</a> | 
<a href="https://youtu.be/H_ef2vGj69s" target="_blank">youtube</a>
<br><br> 

<b>Stress-Testing Point Cloud Registration on Automotive LiDAR</b><br>
Amnon Drory, Raja Giryes, Shai Avidan<br>
<a href="https://ml4ad.github.io/files/papers2022/Stress-Testing Point Cloud Registration on Automotive LiDAR.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D1</a> | 
<a href="https://slideslive.com/38993721/stresstesting-point-cloud-registration-on-automotive-lidar" target="_blank">slideslive</a> | 
<a href="https://youtu.be/YeHrck-a3Hw" target="_blank">youtube</a>
<br><br> 

<b>KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients</b><br>
Niklas Hanselmann, Katrin Renz, Kashyap Chitta, Apratim Bhattacharyya, Andreas Geiger<br>
<a href="https://ml4ad.github.io/files/papers2022/KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D7</a> | 
<a href="https://slideslive.com/38993735/king-generating-safetycritical-driving-scenarios-for-robust-imitation-via-kinematics-gradients" target="_blank">slideslive</a> | 
<a href="https://youtu.be/2uAHPRaWZYk" target="_blank">youtube</a>
<br><br> 

<b>Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception</b><br>
Bin Huang*, Yangguang Li*, Enze Xie*, Feng Liang*, Luya Wang, Mingzhu Shen, Fenggang Liu, Tianqi Wang, Ping Luo, Jing Shao<br>
<a href="https://ml4ad.github.io/files/papers2022/Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D8</a> | 
<a href="https://slideslive.com/38993733/fastbev-towards-realtime-onvehicle-birdseye-view-perception" target="_blank">slideslive</a>
<br><br> 

<b>DiffStack: A Differentiable and Modular Control Stack for Autonomous Vehicles</b><br>
Peter Karkus, Boris Ivanovic, Shie Mannor, Marco Pavone<br>
<a href="https://ml4ad.github.io/files/papers2022/DiffStack: A Differentiable and Modular Control Stack for Autonomous Vehicles.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D5</a> | 
<a href="https://slideslive.com/38993722/diffstack-a-differentiable-and-modular-control-stack-for-autonomous-vehicles" target="_blank">slideslive</a> | 
<a href="https://youtu.be/aJJqijJ6OAc" target="_blank">youtube</a>
<br><br> 

<b>An Intelligent Modular Real-Time Vision-Based System for Environment Perception</b><br>
Amirhossein Kazerouni*, Amirhossein Heydarian*, Milad Soltany*, Aida Mohammadshahi*, Abbas Omidi*, Saeed Ebadollahi<br>
<a href="https://ml4ad.github.io/files/papers2022/An Intelligent Modular Real-Time Vision-Based System for Environment Perception.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A9</a> | 
<a href="https://slideslive.com/38993734/an-intelligent-modular-realtime-visionbased-system-for-environment-perception" target="_blank">slideslive</a> | 
<a href="https://youtu.be/MozBUZEyPao" target="_blank">youtube</a>
<br><br> 

<b>PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?</b><br>
Aleksandr Kim, Guillem Brasó, Aljoša Ošep, Laura Leal-Taixé<br>
<a href="https://ml4ad.github.io/files/papers2022/PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B6</a> | 
<a href="https://slideslive.com/38993730/polarmot-how-far-can-geometric-relations-take-us-in-3d-multiobject-tracking" target="_blank">slideslive</a> | 
<a href="https://youtu.be/59D7ndcz0Xg" target="_blank">youtube</a>
<br><br> 

<b>TALISMAN: Targeted Active Learning for Object Detection with Rare Classes and Slices using Submodular Mutual Information</b><br>
Suraj Kothawade, Saikat Ghosh, Sumit Shekhar, Yu Xiang, Rishabh Iyer<br>
<a href="https://ml4ad.github.io/files/papers2022/TALISMAN: Targeted Active Learning for Object Detection with Rare Classes and Slices using Submodular Mutual Information.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D2</a> | 
<a href="https://slideslive.com/38993703/talisman-targeted-active-learning-for-object-detection-with-rare-classes-and-slices-using-submodular-mutual-information" target="_blank">slideslive</a> | 
<a href="https://youtu.be/ffWr5H3BrfA" target="_blank">youtube</a>
<br><br> 

<b>CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization</b><br>
Eesha Kumar, Yiming Zhang, Stefano Pini, Simon Stent, Ana Sofia Rufino Ferreira, Sergey Zagoruyko, Christian Perone<br>
<a href="https://ml4ad.github.io/files/papers2022/CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted Empirical Risk Minimization.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A4</a> | 
<a href="https://slideslive.com/38993720/cwerm-improving-autonomous-driving-planning-with-closedloop-weighted-empirical-risk-minimization" target="_blank">slideslive</a>
<br><br> 

<b>Missing Traffic Data Imputation Using Multi-Trajectory Parameter Transferred LSTM</b><br>
Jungmin Kwon, Hyunggon Park<br>

<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D3</a> | 
<a href="https://slideslive.com/38993713/missing-traffic-data-imputation-using-multitrajectory-parameter-transferred-lstm" target="_blank">slideslive</a> | 
<a href="https://youtu.be/NgEDy4P-PXU" target="_blank">youtube</a>
<br><br> 

<b>Calibrated Perception Uncertainty Across Objects and Regions in Bird's-Eye-View</b><br>
Markus Kängsepp, Meelis Kull<br>
<a href="https://ml4ad.github.io/files/papers2022/Calibrated Perception Uncertainty Across Objects and Regions in Bird's-Eye-View.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D4</a> | 
<a href="https://slideslive.com/38993729/calibrated-perception-uncertainty-across-objects-and-regions-in-birdseyeview" target="_blank">slideslive</a> | 
<a href="https://youtu.be/eX_hgLUMakk" target="_blank">youtube</a>
<br><br> 

<b>Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios</b><br>
Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Rebecca Roelofs, Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, Dragomir Anguelov, Sergey Levine<br>
<a href="https://ml4ad.github.io/files/papers2022/Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B4</a> | 
<a href="https://slideslive.com/38993705/imitation-is-not-enough-robustifying-imitation-with-reinforcement-learning-for-challenging-driving-scenarios" target="_blank">slideslive</a>
<br><br> 

<b>ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection</b><br>
Yunsheng Ma, Ziran Wang<br>
<a href="https://ml4ad.github.io/files/papers2022/ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A3</a> | 
<a href="https://slideslive.com/38993717/vitdd-multitask-vision-transformer-for-semisupervised-driver-distraction-detection" target="_blank">slideslive</a> | 
<a href="https://youtu.be/UnBLg421xuA" target="_blank">youtube</a>
<br><br> 

<b>Direct LiDAR-based Object Detector Training from Automated 2D Detections</b><br>
Robert McCraith, Eldar Insafutdinov, Lukas Neumann, Andrea Vedaldi<br>
<a href="https://ml4ad.github.io/files/papers2022/Direct LiDAR-based Object Detector Training from Automated 2D Detections.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D9</a> | 
<a href="https://slideslive.com/38993724/direct-lidarbased-object-detector-training-from-automated-2d-detections" target="_blank">slideslive</a>
<br><br> 

<b>Safe Real-World Autonomous Driving by Learning to Predict and Plan with a Mixture of Experts</b><br>
Stefano Pini, Christian Perone, Aayush Ahuja, Ana Sofia Rufino Ferreira, Moritz Niendorf, Sergey Zagoruyko<br>
<a href="https://ml4ad.github.io/files/papers2022/Safe Real-World Autonomous Driving by Learning to Predict and Plan with a Mixture of Experts.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C7</a> | 
<a href="https://slideslive.com/38993714/safepathnet-safe-realworld-autonomous-driving-by-learning-to-predict-and-plan-with-a-mixture-of-experts" target="_blank">slideslive</a>
<br><br> 

<b>PlanT: Explainable Planning Transformers via Object-Level Representations</b><br>
Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, Almut Sophia Koepke, Zeynep Akata, Andreas Geiger<br>
<a href="https://ml4ad.github.io/files/papers2022/PlanT: Explainable Planning Transformers via Object-Level Representations.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C3</a> | 
<a href="https://slideslive.com/38993707/plant-explainable-planning-transformers-via-objectlevel-representations" target="_blank">slideslive</a> | 
<a href="https://youtu.be/rlBQtGUbWYE" target="_blank">youtube</a>
<br><br> 

<b>Distortion-Aware Network Pruning and Feature Reuse for Real-time Video Segmentation</b><br>
Hyunsu Rhee, Dongchan Min, Sunil Hwang, Bruno Andreis, Sung Ju Hwang<br>
<a href="https://ml4ad.github.io/files/papers2022/Distortion-Aware Network Pruning and Feature Reuse for Real-time Video Segmentation.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A10</a> | 
<a href="https://slideslive.com/38993704/distortionaware-network-pruning-and-feature-reuse-for-realtime-video-segmentation" target="_blank">slideslive</a> | 
<a href="https://youtu.be/4qkeu9umwXQ" target="_blank">youtube</a>
<br><br> 

<b>Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation</b><br>
Timo Sämann, Ahmed Mostafa Hammam, Andrei Bursuc, Christoph Stiller, Horst-Michael Groß<br>
<a href="https://ml4ad.github.io/files/papers2022/Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C4</a> | 
<a href="https://slideslive.com/38993726/improving-predictive-performance-and-calibration-by-weight-fusion-in-semantic-segmentation" target="_blank">slideslive</a>
<br><br> 

<b>GNM: A General Navigation Model to Drive Any Robot</b><br>
Dhruv Shah*, Ajay Sridhar*, Arjun Bhorkar, Noriaki Hirose, Sergey Levine<br>
<a href="https://ml4ad.github.io/files/papers2022/GNM: A General Navigation Model to Drive Any Robot.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster D10</a> | 
<a href="https://slideslive.com/38993702/gnm-a-general-navigation-model-to-drive-any-robot" target="_blank">slideslive</a>
<br><br> 

<b>Enhancing System-level Safety in Autonomous Driving via Feedback Learning</b><br>
Sin Yong Tan, Weisi Fan, Qisai Liu, Tichakorn Wongpiromsarn, Soumik Sarkar<br>
<a href="https://ml4ad.github.io/files/papers2022/Enhancing System-level Safety in Autonomous Driving via Feedback Learning.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C10</a> | 
<a href="https://slideslive.com/38993712/enhancing-systemlevel-safety-in-autonomous-driving-via-feedback-learning" target="_blank">slideslive</a> | 
<a href="https://youtu.be/vLWjMrrfhCE" target="_blank">youtube</a>
<br><br> 

<b>Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception</b><br>
Marc Uecker, Tobias Fleck, Marcel Pflugfelder, Marius Zöllner<br>
<a href="https://ml4ad.github.io/files/papers2022/Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C6</a> | 
<a href="https://slideslive.com/38993709/analyzing-deep-learning-representations-for-realtime-invehicle-lidar-perception" target="_blank">slideslive</a>
<br><br> 

<b>CAMEL: Learning Cost-maps Made Easy for Off-road Driving</b><br>
Kasi Viswanath, Sujit Baliyarasimhuni, Srikanth Saripalli<br>
<a href="https://ml4ad.github.io/files/papers2022/CAMEL: Learning Cost-maps Made Easy for Off-road Driving.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster A8</a> | 
<a href="https://slideslive.com/38993732/camel-learning-costmaps-made-easy-for-offroad-driving" target="_blank">slideslive</a> | 
<a href="https://youtu.be/vc5F61COygQ" target="_blank">youtube</a>
<br><br> 

<b>A Versatile and Efficient Reinforcement Learning Approach for Autonomous Driving</b><br>
Guan Wang, Haoyi Niu, Desheng Zhu, Jianming Hu, Xianyuan Zhan, Guyue Zhou<br>
<a href="https://ml4ad.github.io/files/papers2022/A Versatile and Efficient Reinforcement Learning Approach for Autonomous Driving.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C5</a> | 
<a href="https://slideslive.com/38993739/a-versatile-and-efficient-reinforcement-learning-approach-for-autonomous-driving" target="_blank">slideslive</a>
<br><br> 

<b>Uncertainty-Aware Self-Training with Expectation Maximization Basis Transformation</b><br>
Zijia Wang, Wenbin Yang, Zhi-Song Liu, Zhen Jia<br>
<a href="https://ml4ad.github.io/files/papers2022/Uncertainty-Aware Self-Training with Expectation Maximization Basis Transformation.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B10</a> | 
<a href="https://slideslive.com/38993715/uncertaintyaware-selftraining-with-expectation-maximization-basis-transformation" target="_blank">slideslive</a> | 
<a href="https://youtube.com/shorts/-Jb7Eha_1cg" target="_blank">youtube</a>
<br><br> 

<b>Potential Energy based Mixture Model for Noisy Label Learning</b><br>
Zijia Wang, Wenbin Yang, Zhi-Song Liu, Zhen Jia<br>
<a href="https://ml4ad.github.io/files/papers2022/Potential Energy based Mixture Model for Noisy Label Learning.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster C1</a> | 
<a href="https://slideslive.com/38993706/potential-energy-based-mixture-model-for-noisy-label-learning" target="_blank">slideslive</a> | 
<a href="https://youtu.be/JTrOavVJfZI" target="_blank">youtube</a>
<br><br> 

<b>A Graph Representation for Autonomous Driving</b><br>
Zerong Xi, Gita Sukthankar<br>
<a href="https://ml4ad.github.io/files/papers2022/A Graph Representation for Autonomous Driving.pdf" target="_blank">paper</a> | 
<a href="https://app.gather.town/app/ul3nGoeZXMhlgvLJ/ML4AD%202022" target="_blank">poster B8</a> | 
<a href="https://slideslive.com/38993731/a-graph-representation-for-autonomous-driving" target="_blank">slideslive</a> | 
<a href="https://youtu.be/_--tjPruB_4" target="_blank">youtube</a>
<br><br> 

</p>
-->








          </section>

<!--        
          <section class="location" id="location">
            <h2 class="section-title">Location</h2>



<p>
Theater B, 2nd floor</br>
<a href="https://goo.gl/maps/pw94qiZcnub46nWp9" target="_blank">New Orleans Convention Center</a>,  900 Convention Center Boulevard, New Orleans, Louisiana 70130, United States
</p>

<iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d27660.093004164428!2d-90.063431!3d29.935959!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0x185d4d6c82fa192f!2sNew%20Orleans%20Ernest%20N.%20Morial%20Convention%20Center!5e0!3m2!1sen!2sau!4v1662541769305!5m2!1sen!2sau" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>

<p><br><img src="https://ml4ad.github.io/img/floor-plan.jpg" width="600"></p>


          </section>
-->


          <section class="organizers" id="organizers">
            <div class="row">
<div class="column" id="conference-organizers">
<h3 class="section-title">Organizers</h3>
<ul class="organizers-list">

  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="yantianzha.github.io/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/YantianZha.png" alt="Yantian Zha" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="https://yantianzha.github.io/" target="_blank" title="Yantian Zha"> Yantian Zha </a>
    &nbsp;&nbsp; ytzha@umd.edu
    </h3>
    <p class="organizers-bio">is a Postdoctoral Associate at the University of Maryland, College Park. His research focuses on how the learning of different levels of cognitive functions can be tightly coupled together based on cognitive science. Recently, he is actively researching self-explanation-guided robot learning, robot learning with humans in the loop, tactile sensing, and hyper-dimensional computing for robotics. In his early research, he works on plan recognition and explainable robotics. </p>
  </li>

  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="https://cs.gmu.edu/~xiao/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/XuesuXiao.jpg" alt="Xuesu Xiao" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="https://cs.gmu.edu/~xiao/" target="_blank" title="Xuesu Xiao"> Xuesu Xiao   </a>
    &nbsp;&nbsp; xiao@gmu.edu
    </h3>
    <p class="organizers-bio">is an Assistant Professor at George Mason University, USA. He is interested in highly capable and intelligent mobile robotics. He used to be the chair of IJCAI 2023 Robot Exhibition, IEEE ICRA 2022 Competition The Benchmark Autonomous Robot Navigation (BARN) Challenge, AAAI Spring Symposium Series 2021 Machine Learning for Mobile Robot Navigation in the Wild, IEEE ICRA 2021 Workshop Machine Learning for Motion Planning. In addition, he was the organizing committee member in CoRL 2022 Workshop Learning for Agile Robotics, ACM/IEEE HRI 2021 Workshop Exploring Applications for Autonomous Non-Verbal Human-Robot Interactions, ACM/IEEE HRI 2021 Workshop Exploring Applications for Autonomous Non-Verbal Human-Robot Interactions.</p>
  </li>

  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="https://nms.kcl.ac.uk/oya.celiktutan/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/oyaceliktutan.JPG" alt="Oya Celiktutan" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="https://nms.kcl.ac.uk/oya.celiktutan/" target="_blank" title="Oya Celiktutan"> Oya Celiktutan </a>
    <!--&nbsp;&nbsp; oya.celiktutan@kcl.ac.uk-->
    </h3>
    <p class="organizers-bio">is an Associate Professor of Robotics at King's College London, UK. She is interested in machine learning and its application to computer vision, human behaviour understanding and generation, robot learning from and for interactions with humans, and human-machine interaction in general. To date, she (co-)organised more than 10 workshops, special sessions, and challenges. Most recently, she is a co-organiser of MASSXR Workshop at VR 2023, ITAH Workshop at IUI 2023, and SOLAR Workshop as well as METRICS HEART-MET Physically Assistive Robot Challenge at ICRA 2023. She is a doctoral consortium co-chair of ACII 2023 and FG 2024 and a publication co-chair of ICMI 2024.</p>
  </li>


  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="http://www.pengg-robotics.com/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/PengGao.jpeg" alt="Peng Gao" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="http://www.pengg-robotics.com/" target="_blank" title="Peng Gao"> Peng Gao </a>
    <!--&nbsp;&nbsp; gaopeng@umd.edu-->
    </h3>
    <p class="organizers-bio">is a postdoctoral associate working at the University of Maryland College Park. His research focuses on collaborative perception to enable team awareness for multirobot and human-autonomy teaming, so that humans and robots can build shared team awareness and collaboratively complete complex tasks as a team.</p>
  </li>

  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="https://akshararai.github.io/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/AksharaRai‬.jpeg" alt="Akshara Rai‬" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="https://akshararai.github.io/" target="_blank" title="Akshara Rai‬"> Akshara Rai‬ </a>
    <!--&nbsp;&nbsp; akshararai@meta.com-->
    </h3>
    <p class="organizers-bio">is a Research scientist at Meta AI, working at the intersection of machine learning and control for robotics. Her research aims at teaching robots to perform complex, novel tasks in the real-world sample-efficiently, by utilizing prior knowledge in the form of models, simulators, or demonstration data. She has co-organized the following workshops: Perceptive legged locomotion at IROS 2021 and IROS 2022, Retrospectives in robotics at RSS 2020, Sim-to-real in robotics beyond locomotion at CoRL 2022. She has also been AE for robotics conferences like ICRA, and RAL.</p>
  </li>
  
  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="https://www.cs.binghamton.edu/~szhang/" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/ShiqiZhang.jpg" alt="Shiqi Zhang" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="https://www.cs.binghamton.edu/~szhang/" target="_blank" title="Shiqi Zhang"> Shiqi Zhang </a>
    <!--&nbsp;&nbsp; zhangs@binghamton.edu-->
    </h3>
    <p class="organizers-bio">is an Assistant Professor of Computer Science, The State University of New York at Binghamton. He works on AI and robotics, and develops intelligent mobile robots that are able to interact with people, provide services to people, and learn from this experience. He has co-organized the RSS-2021 Workshop on Declarative Knowledge in Learning and Control of Robot Behaviors, two IROS workshops about robot learning and reasoning in 2019 and 2020 respectively, and an AAAI-2018 Spring Symposium. He was a Publication Co-chair of the AAMAS-2022 conference, and is a Co-chair of the KR-2023 Conference’s Special Session on Robotics and Planning.</p>
  </li>


  <br>
  <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
    <span class="organizer-photo">
      <a href="www.martinazambelli.site" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/MartinaZambelli.jpg" alt="Martina Zambelli" itemprop="image"></a>
    </span>

    <h3 class="organizers-name"><a href="www.martinazambelli.site" target="_blank" title="Martina Zambelli"> Martina Zambelli </a>
    <!--&nbsp;&nbsp; zambellim@google.com-->
    </h3>
    <p class="organizers-bio">is a research scientist at Google DeepMind, UK, working at the intersection of robot learning, robot manipulation, and multimodal learning. She completed her Ph.D. at Imperial College London, UK, where she explored the developmental robotic approach for autonomous discovery of sensorimotor capabilities. She is one of the organizers of the Mediterranean Machine Learning school, and she co-organized the Women in ML and Robotics event at CoRL 2021.</p>
  </li>
  
</ul>
</div>

<div class="column" id="workshop-organizers">

  <h3 class="section-title">Advisory Board</h3>
  <ul class="organizers-list">
  
    <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
      <span class="organizer-photo">
        <a href="https://research.manchester.ac.uk/en/persons/angelo.cangelosi" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/AngeloCangelosi.jpeg" alt="Angelo Cangelosi" itemprop="image"></a>
      </span>

      <h3 class="organizers-name"><a href="https://research.manchester.ac.uk/en/persons/angelo.cangelosi" target="_blank" title="Angelo Cangelosi"> Angelo Cangelosi </a>
      <!--&nbsp;&nbsp; angelo.cangelosi@manchester.ac.uk-->
      </h3>
      <p class="organizers-bio">Dr. Angelo Cangelosi is Professor of Machine Learning and Robotics at the University of Manchester (UK). He also is Turing Fellow at the Alan Turing Institute. He has chaired numerous international conferences, including ICANN2022 Bristol, and ICDL2021 Beijing. His book “Developmental Robotics: From Babies to Robots” (MIT Press) was published in January 2015, and translated in Chinese and Japanese. His latest book “Cognitive Robotics” (MIT Press), coedited with Minoru Asada, was recently published in 2022.</p>
    </li>
    
    <li class="organizers-item" itemprop="performer" itemscope itemtype="http://schema.org/Person">
      <span class="organizer-photo">
        <a href="https://en.wikipedia.org/wiki/Minoru_Asada" target="_blank"><img class="photo" src="https://yantianzha.github.io/crl.github.io/img/MinoruAsada.jpg" alt="Minoru Asada" itemprop="image"></a>
      </span>

      <h3 class="organizers-name"><a href="http://www.er.ams.eng.osaka-u.ac.jp/asadalab/?page_id=330&lang=en" target="_blank" title="Minoru Asada"> Minoru Asada </a>
      <!--&nbsp;&nbsp; asada@otri.osaka-u.ac.jp-->
      </h3>
      <p class="organizers-bio">Minoru Asada received B.E., M.E., and Ph.D. degrees in control engineering from Osaka University, Osaka, Japan, in 1977, 1979, and 1982, respectively. In April 1995, he became a professor at Osaka University. He was a professor in the Department of Adaptive Machine Systems at the Graduate School of Engineering, Osaka University from April 1997 to March 2019.  Since then, he has been a specially-appointed professor, Symbiotic Intelligent System Research Center, Open and Transdisciplinary Research Initiatives, Osaka University. From April 2021, he bcame a vice president of International Professional University of Technology in Osaka. Dr. Asada has received many awards, such as the Best Paper award at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS92) and a Commendation by the Minister of Education, Culture, Sports, Science and Technology, Japanese Government, as a Person of Distinguished Services to Enlightening People on Science and Technology. He is one of the founders of the RoboCup, and the former president of the International RoboCup Federation (2002-2008). He was a president for the Robotics Society of Japan (RSJ, 2019-2021). IEEE Life Fellow since 2021. He was the Research Director of the ASADA Synergistic Intelligence Project at Exploratory Research for Advanced Technology by the Japan Science and Technology Agency (ERATO, 2005-2011), and was a principal investigator of the Grants-in-Aid for Scientific Research (Research Project Numbers:24000012, 2012-2016) titled Constructive Developmental Science based on Understanding the Process from Neuro-Dynamics to Social Interaction. He was a principal investigator of the JST RISTEX R&D Project titled Legal Beings: Electric personhoods of artificial intelligence and robots in NAJIMI society, based on a reconsideration of the concept of autonomy.</p>
    </li>
  
  </ul>

  

</div>
</div>




<div class="row">
<span id="pc0">
 <h3 class="section-title">Program Committee</h3>
 <p>&nbsp;&nbsp;&nbsp;We thank those who help make this workshop possible!</p>
</span>
<!--
<div class="column" id="pc1">

  <div class="row">
  <div class="column">
    <p>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?user=BDxnFV0AAAAJ" target="_blank" title="Aayush Ahuja"> Aayush Ahuja </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://dblp.org/pid/270/0255.html" target="_blank" title="Maram Akila"> Maram Akila </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://thegoodai.co/talent/elahe-arani/" target="_blank" title="Elahe Arani"> Elahe Arani </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?user=MpO6-QsAAAAJ" target="_blank" title="Bahar Azari"> Bahar Azari </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.ca/citations?user=-Gt6H80AAAAJ" target="_blank" title="Junchi Bin"> Junchi Bin </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://www.aifb.kit.edu/web/Daniel_Bogdoll" target="_blank" title="Daniel Bogdoll"> Daniel Bogdoll </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://dblp.org/pid/165/5066.html" target="_blank" title="Jaekwang Cha"> Jaekwang Cha </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=OqlY-98AAAAJ" target="_blank" title="Changhao Chen"> Changhao Chen </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://arlenchen.github.io/" target="_blank" title="Zheng Chen"> Zheng Chen </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://wenhao.pub/" target="_blank" title="Wenhao Ding"> Wenhao Ding </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.de/citations?user=8HsbmCMAAAAJ" target="_blank" title="Christopher Diehl"> Christopher Diehl </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://djurikom.github.io/" target="_blank" title="Nemanja Djuric"> Nemanja Djuric </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?user=ubwUAdQAAAAJ" target="_blank" title="Praneet Dutta"> Praneet Dutta </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=Sk-VfXYAAAAJ" target="_blank" title="Hesham Eraqi"> Hesham Eraqi </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?user=VpMNqukAAAAJ" target="_blank" title="Meng Fan"> Meng Fan </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.ca/citations?hl=en&amp;user=ZJgIXd0AAAAJ" target="_blank" title="Shivam Gautam"> Shivam Gautam </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=GbZr2MQAAAAJ" target="_blank" title="Paweł Gora"> Paweł Gora </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://www.linkedin.com/in/simongustavsson-/?originalSubdomain=se" target="_blank" title="Simon Gustavsson"> Simon Gustavsson </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?user=CyzZFw8AAAAJ" target="_blank" title="Michael Hanselmann"> Michael Hanselmann </a>
      
      <br>
    
      &nbsp;•&nbsp; 
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=6OcQSLcAAAAJ" target="_blank" title="Jeffrey Hawke"> Jeffrey Hawke </a>
      
      <br>
    
    </p>
  </div>
  <div class="column">
    <p>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=iVJp94cAAAAJ" target="_blank" title="Silviu Homoceanu"> Silviu Homoceanu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=UyooQDYAAAAJ" target="_blank" title="Hanjiang Hu"> Hanjiang Hu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/francisindaheng/" target="_blank" title="Francis Indaheng"> Francis Indaheng </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=ZYvXHYwAAAAJ" target="_blank" title="Nikita Jaipuria"> Nikita Jaipuria </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.zf.com/" target="_blank" title="Isabel Janez"> Isabel Janez </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://saurabhjha.one/" target="_blank" title="Saurabh Jha"> Saurabh Jha </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=jZgsp_sAAAAJ" target="_blank" title="Ameya Joshi"> Ameya Joshi </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=oLJTcXIAAAAJ" target="_blank" title="Ravi Kiran"> Ravi Kiran </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.co.in/citations?user=epbGOYkAAAAJ" target="_blank" title="Suraj Kothawade"> Suraj Kothawade </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/lee-dongsu-81184b166/" target="_blank" title="Dongsu Lee"> Dongsu Lee </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=weSjLpgAAAAJ" target="_blank" title="Donsuk Lee"> Donsuk Lee </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="http://ailab.kaist.ac.kr/author/khlee" target="_blank" title="Kanghoon Lee"> Kanghoon Lee </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=nhLibMYAAAAJ" target="_blank" title="Johannes Lehner"> Johannes Lehner </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=VbNwxKYAAAAJ" target="_blank" title="Jinning Li"> Jinning Li </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=gIBLutQAAAAJ" target="_blank" title="Zhuwen Li"> Zhuwen Li </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=SULIp-4AAAAJ" target="_blank" title="Xiaoyuan Liang"> Xiaoyuan Liang </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=O2GcOLAAAAAJ" target="_blank" title="Teck Lim"> Teck Lim </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.vlioutas.com/" target="_blank" title="Vasileios Lioutas"> Vasileios Lioutas </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=5ApCTCoAAAAJ" target="_blank" title="Zuxin Liu"> Zuxin Liu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/jun-luo-1098a6126/" target="_blank" title="Jun Luo"> Jun Luo </a>
      
      <br>
    
    </p>
  </div>
  </div>
</div>
<div class="column" id="pc2">
  
  <div class="row">
  <div class="column">
    <p>
    
      &nbsp;•&nbsp;
      
        <a href="https://dblp.org/pid/227/2794.html" target="_blank" title="Hengbo Ma"> Hengbo Ma </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=lqoa8VIAAAAJ" target="_blank" title="Xiaobai Ma"> Xiaobai Ma </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/marcin-mo%C5%BCejko-6b024bbb/" target="_blank" title="Marcin Możejko"> Marcin Możejko </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=aK1eTNkAAAAJ" target="_blank" title="Amitangshu Mukherjee"> Amitangshu Mukherjee </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://sauradip.github.io/" target="_blank" title="Sauradip Nag"> Sauradip Nag </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=y8ODCtcAAAAJ" target="_blank" title="Maximilian Naumann"> Maximilian Naumann </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=38fqeIYAAAAJ" target="_blank" title="Patrick Nguyen"> Patrick Nguyen </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://yaruniu.com/" target="_blank" title="Yaru Niu"> Yaru Niu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=t5Uz7ZIAAAAJ" target="_blank" title="Tanvir Parhar"> Tanvir Parhar </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://nishantrai18.github.io/" target="_blank" title="Nishant Rai"> Nishant Rai </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=BwDylxIAAAAJ" target="_blank" title="Gaurav Raina"> Gaurav Raina </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=fwVWk9UAAAAJ" target="_blank" title="Daniele Reda"> Daniele Reda </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/kasrarezaee/?originalSubdomain=ca" target="_blank" title="Kasra Rezaee"> Kasra Rezaee </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://ennasachdeva.github.io/" target="_blank" title="Enna Sachdeva"> Enna Sachdeva </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=jFrk3WoAAAAJ" target="_blank" title="Mark Schutera"> Mark Schutera </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=Gpw8Z0cAAAAJ" target="_blank" title="Adam Scibior"> Adam Scibior </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://ml4ad.github.io/" target="_blank" title="Daniel Sikar"> Daniel Sikar </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.apoorvsingh.com/" target="_blank" title="Apoorv Singh"> Apoorv Singh </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=eQitgPoAAAAJ" target="_blank" title="Sahib Singh"> Sahib Singh </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=VFesER4AAAAJ" target="_blank" title="Ruobing Shen"> Ruobing Shen </a>
      
      <br>
    
    </p>
  </div>
  <div class="column">
    <p>
    
      &nbsp;•&nbsp;
      
        <a href="https://shitianyu-hue.github.io/" target="_blank" title="Tianyu Shi"> Tianyu Shi </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://people.iith.ac.in/amit_acharyya/Homepages/CV1/Deepak_Kumar.pdf" target="_blank" title="Deepak Singh"> Deepak Singh </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=SODWkygAAAAJ" target="_blank" title="Ibrahim Sobh"> Ibrahim Sobh </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=zqFWRFAAAAAJ" target="_blank" title="Zhaoen Su"> Zhaoen Su </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/shreyassub/" target="_blank" title="Shreyas Subramanian"> Shreyas Subramanian </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=OSkFZZAAAAAJ" target="_blank" title="Ho Suk"> Ho Suk </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://chentangmark.github.io/" target="_blank" title="Chen Tang"> Chen Tang </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.utc.edu/research/center-for-urban-informatics-and-progress/our-team/graduate-students" target="_blank" title="Toan Viet Tran"> Toan Viet Tran </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/ankitvora1/" target="_blank" title="Ankit Vora"> Ankit Vora </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=9WlO88wAAAAJ" target="_blank" title="Ákos Utasi"> Ákos Utasi </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://letianwang0.wixsite.com/myhome" target="_blank" title="Letian Wang"> Letian Wang </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=RpqvaTUAAAAJ" target="_blank" title="Chenfeng Xu"> Chenfeng Xu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=U_R0PWEAAAAJ" target="_blank" title="Yan Xu"> Yan Xu </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=xfskSZEAAAAJ" target="_blank" title="Xinchen Yan"> Xinchen Yan </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/xi-yi-34a4a3a9/" target="_blank" title="Xi Yi"> Xi Yi </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://www.linkedin.com/in/senthilyogamani/" target="_blank" title="Senthil Yogamani"> Senthil Yogamani </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?hl=en&amp;user=Br1mVJQAAAAJ" target="_blank" title="Jiakai Zhang"> Jiakai Zhang </a>
      
      <br>
    
      &nbsp;•&nbsp;
      
        <a href="https://scholar.google.com/citations?user=rKUnBPgAAAAJ" target="_blank" title="Jiacheng Zhu"> Jiacheng Zhu </a>
      
      <br>
    
    </p>
  </div>
  </div>
</div>
-->
</div>



<span id="conference-organizers-below"></span>
<span id="workshop-organizers-below"></span>
<span id="pc0-below"></span>
<span id="pc1-below"></span>
<span id="pc2-below"></span>

<script>
  var x = setTimeout(function() {
    var width = window.innerWidth;
    if (width < 900) {
      document.getElementById("conference-organizers-below").innerHTML = document.getElementById("conference-organizers").innerHTML
      document.getElementById("workshop-organizers-below").innerHTML = document.getElementById("workshop-organizers").innerHTML
      document.getElementById("pc0-below").innerHTML = document.getElementById("pc0").innerHTML
      document.getElementById("pc1-below").innerHTML = document.getElementById("pc1").innerHTML
      document.getElementById("pc2-below").innerHTML = document.getElementById("pc2").innerHTML
      document.getElementById("conference-organizers").innerHTML = ""
      document.getElementById("workshop-organizers").innerHTML = ""
      document.getElementById("pc0").innerHTML = ""
      document.getElementById("pc1").innerHTML = ""
      document.getElementById("pc2").innerHTML = ""
    }
  }, 0);
</script>

          </section>
        
          <section class="sponsors" id="sponsors">
            <h2 class="section-title">Sponsors</h2>

<p>We thank Maryland Robotics Center and DeepMind (intended) for generously sponsoring best paper awards and student travel awards.</p>

<ul class="sponsors-list">

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://robotics.umd.edu/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="https://yantianzha.github.io/crl.github.io/img/UMD.jpeg" alt="MRC" class="photo" itemprop="image">
    </a>
  </li>

  <li class="sponsor-item" itemscope itemtype="http://schema.org/Organization">
    <a href="https://www.deepmind.com/" class="sponsor--link" itemprop="url" target="_blank">
      <img src="https://yantianzha.github.io/crl.github.io/img/deepmind.png" alt="DeepMind" class="photo" itemprop="image">
    </a>
  </li>  

</ul>

          </section>
        

        <footer class="footer">
          </p>

          <p>Website made using <a href="https://github.com/braziljs/conf-boilerplate" target="_blank">Conf Boilerplate</a></p>

          

        </footer>
      </div>
    </div>
  </div>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="https://ml4ad.github.io/js/jquery.js"><\/script>')</script>
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-141810687-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</body>
</html>
